{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053b9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172f69fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Downloads/bbc-text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594c2364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'business', 'sport', 'entertainment', 'politics'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#News Categories\n",
    "pd.unique(df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122a9269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakash\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='category', ylabel='count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6123aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "TOKENIZED_WORDS = []\n",
    "\n",
    "for word in df['text']:\n",
    "    TOKENIZED_WORDS.append(word_tokenize(word.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d866eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tv', 'future', 'in', 'the', 'hands', 'of', 'viewers', 'with', 'home', 'theatre', 'systems', 'plasma', 'high-definition', 'tvs', 'and', 'digital', 'video', 'recorders', 'moving', 'into', 'the', 'living', 'room', 'the', 'way', 'people', 'watch', 'tv', 'will', 'be', 'radically', 'different', 'in', 'five', 'years', 'time', '.', 'that', 'is', 'according', 'to', 'an', 'expert', 'panel', 'which', 'gathered', 'at', 'the', 'annual', 'consumer', 'electronics', 'show', 'in', 'las', 'vegas', 'to', 'discuss', 'how', 'these', 'new', 'technologies', 'will', 'impact', 'one', 'of', 'our', 'favourite', 'pastimes', '.', 'with', 'the', 'us', 'leading', 'the', 'trend', 'programmes', 'and', 'other', 'content', 'will', 'be', 'delivered', 'to', 'viewers', 'via', 'home', 'networks', 'through', 'cable', 'satellite', 'telecoms', 'companies', 'and', 'broadband', 'service', 'providers', 'to', 'front', 'rooms', 'and', 'portable', 'devices', '.', 'one', 'of', 'the', 'most', 'talked-about', 'technologies', 'of', 'ces', 'has', 'been', 'digital', 'and', 'personal', 'video', 'recorders', '(', 'dvr', 'and', 'pvr', ')', '.', 'these', 'set-top', 'boxes', 'like', 'the', 'us', 's', 'tivo', 'and', 'the', 'uk', 's', 'sky+', 'system', 'allow', 'people', 'to', 'record', 'store', 'play', 'pause', 'and', 'forward', 'wind', 'tv', 'programmes', 'when', 'they', 'want', '.', 'essentially', 'the', 'technology', 'allows', 'for', 'much', 'more', 'personalised', 'tv', '.', 'they', 'are', 'also', 'being', 'built-in', 'to', 'high-definition', 'tv', 'sets', 'which', 'are', 'big', 'business', 'in', 'japan', 'and', 'the', 'us', 'but', 'slower', 'to', 'take', 'off', 'in', 'europe', 'because', 'of', 'the', 'lack', 'of', 'high-definition', 'programming', '.', 'not', 'only', 'can', 'people', 'forward', 'wind', 'through', 'adverts', 'they', 'can', 'also', 'forget', 'about', 'abiding', 'by', 'network', 'and', 'channel', 'schedules', 'putting', 'together', 'their', 'own', 'a-la-carte', 'entertainment', '.', 'but', 'some', 'us', 'networks', 'and', 'cable', 'and', 'satellite', 'companies', 'are', 'worried', 'about', 'what', 'it', 'means', 'for', 'them', 'in', 'terms', 'of', 'advertising', 'revenues', 'as', 'well', 'as', 'brand', 'identity', 'and', 'viewer', 'loyalty', 'to', 'channels', '.', 'although', 'the', 'us', 'leads', 'in', 'this', 'technology', 'at', 'the', 'moment', 'it', 'is', 'also', 'a', 'concern', 'that', 'is', 'being', 'raised', 'in', 'europe', 'particularly', 'with', 'the', 'growing', 'uptake', 'of', 'services', 'like', 'sky+', '.', 'what', 'happens', 'here', 'today', 'we', 'will', 'see', 'in', 'nine', 'months', 'to', 'a', 'years', 'time', 'in', 'the', 'uk', 'adam', 'hume', 'the', 'bbc', 'broadcast', 's', 'futurologist', 'told', 'the', 'bbc', 'news', 'website', '.', 'for', 'the', 'likes', 'of', 'the', 'bbc', 'there', 'are', 'no', 'issues', 'of', 'lost', 'advertising', 'revenue', 'yet', '.', 'it', 'is', 'a', 'more', 'pressing', 'issue', 'at', 'the', 'moment', 'for', 'commercial', 'uk', 'broadcasters', 'but', 'brand', 'loyalty', 'is', 'important', 'for', 'everyone', '.', 'we', 'will', 'be', 'talking', 'more', 'about', 'content', 'brands', 'rather', 'than', 'network', 'brands', 'said', 'tim', 'hanlon', 'from', 'brand', 'communications', 'firm', 'starcom', 'mediavest', '.', 'the', 'reality', 'is', 'that', 'with', 'broadband', 'connections', 'anybody', 'can', 'be', 'the', 'producer', 'of', 'content', '.', 'he', 'added', ':', 'the', 'challenge', 'now', 'is', 'that', 'it', 'is', 'hard', 'to', 'promote', 'a', 'programme', 'with', 'so', 'much', 'choice', '.', 'what', 'this', 'means', 'said', 'stacey', 'jolna', 'senior', 'vice', 'president', 'of', 'tv', 'guide', 'tv', 'group', 'is', 'that', 'the', 'way', 'people', 'find', 'the', 'content', 'they', 'want', 'to', 'watch', 'has', 'to', 'be', 'simplified', 'for', 'tv', 'viewers', '.', 'it', 'means', 'that', 'networks', 'in', 'us', 'terms', 'or', 'channels', 'could', 'take', 'a', 'leaf', 'out', 'of', 'google', 's', 'book', 'and', 'be', 'the', 'search', 'engine', 'of', 'the', 'future', 'instead', 'of', 'the', 'scheduler', 'to', 'help', 'people', 'find', 'what', 'they', 'want', 'to', 'watch', '.', 'this', 'kind', 'of', 'channel', 'model', 'might', 'work', 'for', 'the', 'younger', 'ipod', 'generation', 'which', 'is', 'used', 'to', 'taking', 'control', 'of', 'their', 'gadgets', 'and', 'what', 'they', 'play', 'on', 'them', '.', 'but', 'it', 'might', 'not', 'suit', 'everyone', 'the', 'panel', 'recognised', '.', 'older', 'generations', 'are', 'more', 'comfortable', 'with', 'familiar', 'schedules', 'and', 'channel', 'brands', 'because', 'they', 'know', 'what', 'they', 'are', 'getting', '.', 'they', 'perhaps', 'do', 'not', 'want', 'so', 'much', 'of', 'the', 'choice', 'put', 'into', 'their', 'hands', 'mr', 'hanlon', 'suggested', '.', 'on', 'the', 'other', 'end', 'you', 'have', 'the', 'kids', 'just', 'out', 'of', 'diapers', 'who', 'are', 'pushing', 'buttons', 'already', '-', 'everything', 'is', 'possible', 'and', 'available', 'to', 'them', 'said', 'mr', 'hanlon', '.', 'ultimately', 'the', 'consumer', 'will', 'tell', 'the', 'market', 'they', 'want', '.', 'of', 'the', '50', '000', 'new', 'gadgets', 'and', 'technologies', 'being', 'showcased', 'at', 'ces', 'many', 'of', 'them', 'are', 'about', 'enhancing', 'the', 'tv-watching', 'experience', '.', 'high-definition', 'tv', 'sets', 'are', 'everywhere', 'and', 'many', 'new', 'models', 'of', 'lcd', '(', 'liquid', 'crystal', 'display', ')', 'tvs', 'have', 'been', 'launched', 'with', 'dvr', 'capability', 'built', 'into', 'them', 'instead', 'of', 'being', 'external', 'boxes', '.', 'one', 'such', 'example', 'launched', 'at', 'the', 'show', 'is', 'humax', 's', '26-inch', 'lcd', 'tv', 'with', 'an', '80-hour', 'tivo', 'dvr', 'and', 'dvd', 'recorder', '.', 'one', 'of', 'the', 'us', 's', 'biggest', 'satellite', 'tv', 'companies', 'directtv', 'has', 'even', 'launched', 'its', 'own', 'branded', 'dvr', 'at', 'the', 'show', 'with', '100-hours', 'of', 'recording', 'capability', 'instant', 'replay', 'and', 'a', 'search', 'function', '.', 'the', 'set', 'can', 'pause', 'and', 'rewind', 'tv', 'for', 'up', 'to', '90', 'hours', '.', 'and', 'microsoft', 'chief', 'bill', 'gates', 'announced', 'in', 'his', 'pre-show', 'keynote', 'speech', 'a', 'partnership', 'with', 'tivo', 'called', 'tivotogo', 'which', 'means', 'people', 'can', 'play', 'recorded', 'programmes', 'on', 'windows', 'pcs', 'and', 'mobile', 'devices', '.', 'all', 'these', 'reflect', 'the', 'increasing', 'trend', 'of', 'freeing', 'up', 'multimedia', 'so', 'that', 'people', 'can', 'watch', 'what', 'they', 'want', 'when', 'they', 'want', '.']\n"
     ]
    }
   ],
   "source": [
    "#Text is now tokenized\n",
    "for words in TOKENIZED_WORDS[0:1]:\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63810aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling TOKENIZED_WORDS\n",
    "\n",
    "import pickle\n",
    "\n",
    "file = \"Downloads/TOKENIZED_WORDS.pkl\"\n",
    "fileobj = open(file, 'wb')\n",
    "pickle.dump(TOKENIZED_WORDS, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4744398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Text : \n",
      "[['tv', 'future', 'hands', 'viewers', 'home', 'theatre', 'systems', 'plasma', 'high-definition', 'tvs', 'digital', 'video', 'recorders', 'moving', 'living', 'room', 'way', 'people', 'watch', 'tv', 'radically', 'different', 'five', 'years', 'time', 'according', 'expert', 'panel', 'gathered', 'annual', 'consumer', 'electronics', 'show', 'las', 'vegas', 'discuss', 'new', 'technologies', 'impact', 'one', 'favourite', 'pastimes', 'us', 'leading', 'trend', 'programmes', 'content', 'delivered', 'viewers', 'via', 'home', 'networks', 'cable', 'satellite', 'telecoms', 'companies', 'broadband', 'service', 'providers', 'front', 'rooms', 'portable', 'devices', 'one', 'talked-about', 'technologies', 'ces', 'digital', 'personal', 'video', 'recorders', 'dvr', 'pvr', 'set-top', 'boxes', 'like', 'us', 'tivo', 'uk', 'sky+', 'system', 'allow', 'people', 'record', 'store', 'play', 'pause', 'forward', 'wind', 'tv', 'programmes', 'want', 'essentially', 'technology', 'allows', 'much', 'personalised', 'tv', 'also', 'built-in', 'high-definition', 'tv', 'sets', 'big', 'business', 'japan', 'us', 'slower', 'take', 'europe', 'lack', 'high-definition', 'programming', 'people', 'forward', 'wind', 'adverts', 'also', 'forget', 'abiding', 'network', 'channel', 'schedules', 'putting', 'together', 'a-la-carte', 'entertainment', 'us', 'networks', 'cable', 'satellite', 'companies', 'worried', 'means', 'terms', 'advertising', 'revenues', 'well', 'brand', 'identity', 'viewer', 'loyalty', 'channels', 'although', 'us', 'leads', 'technology', 'moment', 'also', 'concern', 'raised', 'europe', 'particularly', 'growing', 'uptake', 'services', 'like', 'sky+', 'happens', 'today', 'see', 'nine', 'months', 'years', 'time', 'uk', 'adam', 'hume', 'bbc', 'broadcast', 'futurologist', 'told', 'bbc', 'news', 'website', 'likes', 'bbc', 'issues', 'lost', 'advertising', 'revenue', 'yet', 'pressing', 'issue', 'moment', 'commercial', 'uk', 'broadcasters', 'brand', 'loyalty', 'important', 'everyone', 'talking', 'content', 'brands', 'rather', 'network', 'brands', 'said', 'tim', 'hanlon', 'brand', 'communications', 'firm', 'starcom', 'mediavest', 'reality', 'broadband', 'connections', 'anybody', 'producer', 'content', 'added', 'challenge', 'hard', 'promote', 'programme', 'much', 'choice', 'means', 'said', 'stacey', 'jolna', 'senior', 'vice', 'president', 'tv', 'guide', 'tv', 'group', 'way', 'people', 'find', 'content', 'want', 'watch', 'simplified', 'tv', 'viewers', 'means', 'networks', 'us', 'terms', 'channels', 'could', 'take', 'leaf', 'google', 'book', 'search', 'engine', 'future', 'instead', 'scheduler', 'help', 'people', 'find', 'want', 'watch', 'kind', 'channel', 'model', 'might', 'work', 'younger', 'ipod', 'generation', 'used', 'taking', 'control', 'gadgets', 'play', 'might', 'suit', 'everyone', 'panel', 'recognised', 'older', 'generations', 'comfortable', 'familiar', 'schedules', 'channel', 'brands', 'know', 'getting', 'perhaps', 'want', 'much', 'choice', 'put', 'hands', 'mr', 'hanlon', 'suggested', 'end', 'kids', 'diapers', 'pushing', 'buttons', 'already', 'everything', 'possible', 'available', 'said', 'mr', 'hanlon', 'ultimately', 'consumer', 'tell', 'market', 'want', '50', '000', 'new', 'gadgets', 'technologies', 'showcased', 'ces', 'many', 'enhancing', 'tv-watching', 'experience', 'high-definition', 'tv', 'sets', 'everywhere', 'many', 'new', 'models', 'lcd', 'liquid', 'crystal', 'display', 'tvs', 'launched', 'dvr', 'capability', 'built', 'instead', 'external', 'boxes', 'one', 'example', 'launched', 'show', 'humax', '26-inch', 'lcd', 'tv', '80-hour', 'tivo', 'dvr', 'dvd', 'recorder', 'one', 'us', 'biggest', 'satellite', 'tv', 'companies', 'directtv', 'even', 'launched', 'branded', 'dvr', 'show', '100-hours', 'recording', 'capability', 'instant', 'replay', 'search', 'function', 'set', 'pause', 'rewind', 'tv', '90', 'hours', 'microsoft', 'chief', 'bill', 'gates', 'announced', 'pre-show', 'keynote', 'speech', 'partnership', 'tivo', 'called', 'tivotogo', 'means', 'people', 'play', 'recorded', 'programmes', 'windows', 'pcs', 'mobile', 'devices', 'reflect', 'increasing', 'trend', 'freeing', 'multimedia', 'people', 'watch', 'want', 'want']]\n"
     ]
    }
   ],
   "source": [
    "#Removing stopwords and punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "FILTERED_TEXT = []\n",
    "\n",
    "for text in TOKENIZED_WORDS:\n",
    "    temp_text = []\n",
    "    for i in text:\n",
    "        if((i not in stop_words) and (i not in punctuations) and (i != \"'s'\")):\n",
    "            temp_text.append(i)\n",
    "            \n",
    "    FILTERED_TEXT.append(temp_text)\n",
    "    \n",
    "print(\"\\nFiltered Text : \")\n",
    "print(FILTERED_TEXT[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b60da4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling FILTERED_TEXT\n",
    "\n",
    "file = \"Downloads/FILTERED_TEXT.pkl\"\n",
    "fileobj = open(file, 'wb')\n",
    "pickle.dump(FILTERED_TEXT, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "154fe897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Text : \n",
      "['tv futur hand viewer home theatr system plasma high-definit tv digit video record move live room way peopl watch tv radic differ five year time accord expert panel gather annual consum electron show la vega discuss new technolog impact one favourit pastim us lead trend programm content deliv viewer via home network cabl satellit telecom compani broadband servic provid front room portabl devic one talked-about technolog ce digit person video record dvr pvr set-top box like us tivo uk sky+ system allow peopl record store play paus forward wind tv programm want essenti technolog allow much personalis tv also built-in high-definit tv set big busi japan us slower take europ lack high-definit program peopl forward wind advert also forget abid network channel schedul put togeth a-la-cart entertain us network cabl satellit compani worri mean term advertis revenu well brand ident viewer loyalti channel although us lead technolog moment also concern rais europ particularli grow uptak servic like sky+ happen today see nine month year time uk adam hume bbc broadcast futurologist told bbc news websit like bbc issu lost advertis revenu yet press issu moment commerci uk broadcast brand loyalti import everyon talk content brand rather network brand said tim hanlon brand commun firm starcom mediavest realiti broadband connect anybodi produc content ad challeng hard promot programm much choic mean said stacey jolna senior vice presid tv guid tv group way peopl find content want watch simplifi tv viewer mean network us term channel could take leaf googl book search engin futur instead schedul help peopl find want watch kind channel model might work younger ipod gener use take control gadget play might suit everyon panel recognis older gener comfort familiar schedul channel brand know get perhap want much choic put hand mr hanlon suggest end kid diaper push button alreadi everyth possibl avail said mr hanlon ultim consum tell market want 50 000 new gadget technolog showcas ce mani enhanc tv-watch experi high-definit tv set everywher mani new model lcd liquid crystal display tv launch dvr capabl built instead extern box one exampl launch show humax 26-inch lcd tv 80-hour tivo dvr dvd record one us biggest satellit tv compani directtv even launch brand dvr show 100-hour record capabl instant replay search function set paus rewind tv 90 hour microsoft chief bill gate announc pre-show keynot speech partnership tivo call tivotogo mean peopl play record programm window pc mobil devic reflect increas trend free multimedia peopl watch want want', 'worldcom boss left book alon former worldcom boss berni ebber accus overse 11bn £5.8bn fraud never made account decis wit told juror david myer made comment question defenc lawyer argu mr ebber respons worldcom problem phone compani collaps 2002 prosecutor claim loss hidden protect firm share mr myer alreadi plead guilti fraud assist prosecutor monday defenc lawyer reid weingarten tri distanc client alleg cross examin ask mr myer ever knew mr ebber make account decis awar mr myer repli ever know mr ebber make account entri worldcom book mr weingarten press repli wit mr myer admit order fals account entri request former worldcom chief financi offic scott sullivan defenc lawyer tri paint mr sullivan admit fraud testifi later trial mastermind behind worldcom account hous card mr ebber team meanwhil look portray affabl boss admiss pe graduat economist whatev abil mr ebber transform worldcom rel unknown 160bn telecom giant investor darl late 1990 worldcom problem mount howev competit increas telecom boom peter firm final collaps sharehold lost 180bn 20 000 worker lost job mr ebber trial expect last two month found guilti former ceo face substanti jail sentenc firmli declar innoc']\n"
     ]
    }
   ],
   "source": [
    "#Stemmin using Porter Stemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "STEMMED_TEXT = []\n",
    "\n",
    "for text in FILTERED_TEXT:\n",
    "    temp_text = []\n",
    "    for word in text:\n",
    "        temp_text.append(porter.stem(word))\n",
    "        \n",
    "    STEMMED_TEXT.append(\" \".join(temp_text))\n",
    "    \n",
    "print(\"Stemmed Text : \")\n",
    "print(STEMMED_TEXT[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1435ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling STEMMED_TEXT\n",
    "\n",
    "\n",
    "file = \"Downloads/STEMMED_TEXT.pkl\"\n",
    "fileobj = open(file, 'wb')\n",
    "pickle.dump(STEMMED_TEXT, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b06a66ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1c1af57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv futur hand viewer home theatr system plasma...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss left book alon former worldcom b...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger wari farrel gambl leicest say rush make ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yead face newcastl fa cup premiership side new...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean twelv raid box offic ocean twelv crime c...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       category\n",
       "0  tv futur hand viewer home theatr system plasma...           tech\n",
       "1  worldcom boss left book alon former worldcom b...       business\n",
       "2  tiger wari farrel gambl leicest say rush make ...          sport\n",
       "3  yead face newcastl fa cup premiership side new...          sport\n",
       "4  ocean twelv raid box offic ocean twelv crime c...  entertainment"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replacing text with STEMMED_TEXT\n",
    "\n",
    "df = df.drop(['text'], axis=1)\n",
    "df.insert(0, \"text\", STEMMED_TEXT, True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d15292f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>encoded_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv futur hand viewer home theatr system plasma...</td>\n",
       "      <td>tech</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss left book alon former worldcom b...</td>\n",
       "      <td>business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger wari farrel gambl leicest say rush make ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yead face newcastl fa cup premiership side new...</td>\n",
       "      <td>sport</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean twelv raid box offic ocean twelv crime c...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       category  \\\n",
       "0  tv futur hand viewer home theatr system plasma...           tech   \n",
       "1  worldcom boss left book alon former worldcom b...       business   \n",
       "2  tiger wari farrel gambl leicest say rush make ...          sport   \n",
       "3  yead face newcastl fa cup premiership side new...          sport   \n",
       "4  ocean twelv raid box offic ocean twelv crime c...  entertainment   \n",
       "\n",
       "   encoded_category  \n",
       "0                 4  \n",
       "1                 0  \n",
       "2                 3  \n",
       "3                 3  \n",
       "4                 1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Encoding News Category\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "df.insert(2, \"encoded_category\", labelencoder.fit_transform(df['category']), True)\n",
    "\n",
    "df.head()\n",
    "\n",
    "#Business = 0\n",
    "#Entertainment = 1\n",
    "#Politics = 2\n",
    "#Sport = 3\n",
    "#Tech = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc55dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pickling Dataframe\n",
    "\n",
    "file = \"Downloads/DATAFRAME.pkl\"\n",
    "fileobj = open(file, 'wb')\n",
    "pickle.dump(df, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2e530",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77d1aacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>encoded_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv futur hand viewer home theatr system plasma...</td>\n",
       "      <td>tech</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss left book alon former worldcom b...</td>\n",
       "      <td>business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger wari farrel gambl leicest say rush make ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yead face newcastl fa cup premiership side new...</td>\n",
       "      <td>sport</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean twelv raid box offic ocean twelv crime c...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       category  \\\n",
       "0  tv futur hand viewer home theatr system plasma...           tech   \n",
       "1  worldcom boss left book alon former worldcom b...       business   \n",
       "2  tiger wari farrel gambl leicest say rush make ...          sport   \n",
       "3  yead face newcastl fa cup premiership side new...          sport   \n",
       "4  ocean twelv raid box offic ocean twelv crime c...  entertainment   \n",
       "\n",
       "   encoded_category  \n",
       "0                 4  \n",
       "1                 0  \n",
       "2                 3  \n",
       "3                 3  \n",
       "4                 1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"Downloads/DATAFRAME.pkl\"\n",
    "fileobj = open(file, 'rb')\n",
    "df = pickle.load(fileobj)\n",
    "fileobj.close()\n",
    "\n",
    "print(type(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9f2af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#News text\n",
    "X = df['text']\n",
    "\n",
    "#Encoded News Category\n",
    "y = df['encoded_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db102383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data set into Training and Testing set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Testing_set = 25% and Training_set = 75%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7df49dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X : (2225,)\n",
      "Shape of y : (2225,)\n",
      "\n",
      "Shape of X_train : (1668,)\n",
      "Shape of y_train : (1668,)\n",
      "Shape of X_test : (557,)\n",
      "Shape of y_test : (557,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X : \" + str(X.shape))\n",
    "print(\"Shape of y : \" + str(y.shape))\n",
    "\n",
    "print(\"\\nShape of X_train : \" + str(X_train.shape))\n",
    "print(\"Shape of y_train : \" + str(y_train.shape))\n",
    "print(\"Shape of X_test : \" + str(X_test.shape))\n",
    "print(\"Shape of y_test : \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91726aa",
   "metadata": {},
   "source": [
    "# Feature Selection : TF-IDF Approach\n",
    "Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80a88d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Instantiating TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "#Fitting and Transforming Taining Data(X_train)\n",
    "tfidf_X_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "#Tramsforming Testing Data(X_test)\n",
    "tfidf_X_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "#Saving tfidf_vectorizer\n",
    "pickle.dump(tfidf_vectorizer, open(\"Downloads/tfidf_vectorizer.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8605c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multimomial Naive Bayes Classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Instantiating Naive Bayes Classifier with alpha = 1.0\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "#Fitting nb_classifier to Training Data\n",
    "nb_classifier.fit(tfidf_X_train, y_train)\n",
    "\n",
    "#Saving nb_classifier for tfidf_vectorizer\n",
    "pickle.dump(nb_classifier, open(\"Downloads/nb_classifier_for_tfidf_vectorizer.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6c9bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nb_classifier.predict(tfidf_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62b6f27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes : (TF-IDF Approach) \n",
      "\n",
      "Accuracy : 98.56 %\n",
      "\n",
      "Confusion Matrix : \n",
      "[[132   0   3   0   1]\n",
      " [  1  92   0   0   2]\n",
      " [  1   0 110   0   0]\n",
      " [  0   0   0 123   0]\n",
      " [  0   0   0   0  92]]\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Score and Confusion Matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Multinomial Naive Bayes : (TF-IDF Approach) \\n\")\n",
    "\n",
    "#Accuracy\n",
    "a_score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy : \" + str(\"{:.2f}\".format(a_score*100)), '%\\n')\n",
    "\n",
    "#Confusion MAtrix\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, pred)\n",
    "\n",
    "print(\"Confusion Matrix : \")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e62fde6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha :  0.0\n",
      "Accuracy Score :  0.9605026929982047\n",
      "\n",
      "Alpha :  0.1\n",
      "Accuracy Score :  0.9784560143626571\n",
      "\n",
      "Alpha :  0.2\n",
      "Accuracy Score :  0.9856373429084381\n",
      "\n",
      "Alpha :  0.30000000000000004\n",
      "Accuracy Score :  0.9856373429084381\n",
      "\n",
      "Alpha :  0.4\n",
      "Accuracy Score :  0.9856373429084381\n",
      "\n",
      "Alpha :  0.5\n",
      "Accuracy Score :  0.9856373429084381\n",
      "\n",
      "Alpha :  0.6000000000000001\n",
      "Accuracy Score :  0.9856373429084381\n",
      "\n",
      "Alpha :  0.7000000000000001\n",
      "Accuracy Score :  0.9856373429084381\n",
      "\n",
      "Alpha :  0.8\n",
      "Accuracy Score :  0.9856373429084381\n",
      "\n",
      "Alpha :  0.9\n",
      "Accuracy Score :  0.9856373429084381\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakash\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "#Laplace Smooting (Tunning parameter - alpha)\n",
    "\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "#Function for traing nb_classifier with differnt alpha values\n",
    "def train_predict(alpha):\n",
    "    #Instantiating Naive Bayes Classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    \n",
    "    #Fitting nb_classifier to traning data\n",
    "    nb_classifier.fit(tfidf_X_train, y_train)\n",
    "    \n",
    "    #Prediction\n",
    "    pred = nb_classifier.predict(tfidf_X_test)\n",
    "    \n",
    "    #Accuracy Score\n",
    "    a_score = metrics.accuracy_score(y_test, pred)\n",
    "    \n",
    "    return a_score\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha : \", alpha)\n",
    "    print(\"Accuracy Score : \", train_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee68d5",
   "metadata": {},
   "source": [
    "# Feature Selection : Bag of Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf6e3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feture Extraction\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Instantiating CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "#Fitting and Transforming Taining Data(X_train)\n",
    "count_X_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "#Transforming Testing Data (X_test)\n",
    "count_X_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "#Saving count_vectorizer\n",
    "pickle.dump(count_vectorizer, open(\"Downloads/count_vectorizer.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "577f1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Naive Bayes Classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Instantiating Naive Bayes Classifier with alpha = 1.0\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "#Fitting nb_classifier to Training Data\n",
    "nb_classifier.fit(count_X_train, y_train)\n",
    "\n",
    "pickle.dump(nb_classifier, open(\"Downloads/nb_classifier_for_count_vectorizer.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0016af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction \n",
    "pred = nb_classifier.predict(count_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ae43d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes : (BOW Approach) \n",
      "\n",
      "Accuracy : 98.03 %\n",
      "\n",
      "Confusion Matrix : \n",
      "[[130   0   3   0   3]\n",
      " [  0  92   0   0   3]\n",
      " [  1   0 110   0   0]\n",
      " [  0   0   1 122   0]\n",
      " [  0   0   0   0  92]]\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score and Confusion matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Multinomial Naive Bayes : (BOW Approach) \\n\")\n",
    "\n",
    "#Accuracy\n",
    "a_score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy : \" + str(\"{:.2f}\".format(a_score*100)), '%\\n')\n",
    "\n",
    "#Confusion MAtrix\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, pred)\n",
    "\n",
    "print(\"Confusion Matrix : \")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "901858c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha :  0.0\n",
      "Accuracy Score :  0.9730700179533214\n",
      "\n",
      "Alpha :  0.1\n",
      "Accuracy Score :  0.9802513464991023\n",
      "\n",
      "Alpha :  0.2\n",
      "Accuracy Score :  0.9802513464991023\n",
      "\n",
      "Alpha :  0.30000000000000004\n",
      "Accuracy Score :  0.9802513464991023\n",
      "\n",
      "Alpha :  0.4\n",
      "Accuracy Score :  0.9802513464991023\n",
      "\n",
      "Alpha :  0.5\n",
      "Accuracy Score :  0.9802513464991023\n",
      "\n",
      "Alpha :  0.6000000000000001\n",
      "Accuracy Score :  0.9784560143626571\n",
      "\n",
      "Alpha :  0.7000000000000001\n",
      "Accuracy Score :  0.9802513464991023\n",
      "\n",
      "Alpha :  0.8\n",
      "Accuracy Score :  0.9802513464991023\n",
      "\n",
      "Alpha :  0.9\n",
      "Accuracy Score :  0.9802513464991023\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prakash\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    }
   ],
   "source": [
    "#Laplace Smoothing (Tunning parameter - alpha)\n",
    "\n",
    "alphas = np.arange(0,1,0.1)\n",
    "\n",
    "#Function for traing nb_classifier with differnt alpha values\n",
    "def train_predict(alpha):\n",
    "    #Instantiating Naive Bayes Classifier\n",
    "    nb_classifier = MultinomialNB(alpha=alpha)\n",
    "    \n",
    "    #Fitting nb_classifier to traning data\n",
    "    nb_classifier.fit(count_X_train, y_train)\n",
    "    \n",
    "    #Prediction\n",
    "    pred = nb_classifier.predict(count_X_test)\n",
    "    \n",
    "    #Accuracy Score\n",
    "    a_score = metrics.accuracy_score(y_test, pred)\n",
    "    \n",
    "    return a_score\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(\"Alpha : \", alpha)\n",
    "    print(\"Accuracy Score : \", train_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72603641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction of New Category\n",
    "\n",
    "import pickle\n",
    "\n",
    "tfidf_vectorizer = pickle.load(open(\"Downloads/tfidf_vectorizer.pkl\", 'rb'))\n",
    "nb_classifier = pickle.load(open(\"Downloads/nb_classifier_for_tfidf_vectorizer.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcd6aa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the news : Evergrande shares fall 14% as trading resumes in Hong Kong\n",
      "\n",
      "News Category :  Business\n"
     ]
    }
   ],
   "source": [
    "#Values encoded by LabelEncoder\n",
    "encoded = {0:'Business', 1:'Entertainment', 2:'Politics', 3:'Sports', 4:'Technology'}\n",
    "\n",
    "#Input\n",
    "user_text = [input(\"Enter the news : \")]\n",
    "\n",
    "#Transformation and Prediction of user text\n",
    "count = count_vectorizer.transform(user_text)\n",
    "prediction = nb_classifier.predict(count)\n",
    "\n",
    "print(\"\\nNews Category : \", encoded[prediction[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54839c9d",
   "metadata": {},
   "source": [
    "# Support Vector Machine Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "def291d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2ae34aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>encoded_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv futur hand viewer home theatr system plasma...</td>\n",
       "      <td>tech</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss left book alon former worldcom b...</td>\n",
       "      <td>business</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tiger wari farrel gambl leicest say rush make ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yead face newcastl fa cup premiership side new...</td>\n",
       "      <td>sport</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean twelv raid box offic ocean twelv crime c...</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       category  \\\n",
       "0  tv futur hand viewer home theatr system plasma...           tech   \n",
       "1  worldcom boss left book alon former worldcom b...       business   \n",
       "2  tiger wari farrel gambl leicest say rush make ...          sport   \n",
       "3  yead face newcastl fa cup premiership side new...          sport   \n",
       "4  ocean twelv raid box offic ocean twelv crime c...  entertainment   \n",
       "\n",
       "   encoded_category  \n",
       "0                 4  \n",
       "1                 0  \n",
       "2                 3  \n",
       "3                 3  \n",
       "4                 1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading dataframe\n",
    "\n",
    "import pickle\n",
    "\n",
    "file = \"Downloads/DATAFRAME.pkl\"\n",
    "fileobj = open(file, 'rb')\n",
    "df = pickle.load(fileobj)\n",
    "fileobj.close()\n",
    "\n",
    "print(type(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de48bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#News Headlines\n",
    "X = df['text']\n",
    "\n",
    "#Encoded News Category\n",
    "y = df['encoded_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6938e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into Training set & Testing set\n",
    "\n",
    "#Required Library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#testing_set = 25% nd Training_set = 75%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c50a31a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X : (2225,)\n",
      "Shape of y : (2225,)\n",
      "\n",
      "Shape of X_train : (1668,)\n",
      "Shape of y_train : (1668,)\n",
      "Shape of X_test : (557,)\n",
      "Shape of y_test : (557,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X : \" + str(X.shape))\n",
    "print(\"Shape of y : \" + str(y.shape))\n",
    "\n",
    "print(\"\\nShape of X_train : \" + str(X_train.shape))\n",
    "print(\"Shape of y_train : \" + str(y_train.shape))\n",
    "print(\"Shape of X_test : \" + str(X_test.shape))\n",
    "print(\"Shape of y_test : \" + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ca168",
   "metadata": {},
   "source": [
    "# Feature Selection : TF-IDF Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a24f6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "# Required Library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Loading Fitted TfidfVectorizer\n",
    "tfidf_vectorizer = pickle.load(open(\"Downloads/tfidf_vectorizer.pkl\", \"rb\"))\n",
    "\n",
    "# Transforming Training Data (X_train)\n",
    "tfidf_X_train = tfidf_vectorizer.transform(X_train.values)\n",
    "\n",
    "# Transforming Testing Dara (X_test)\n",
    "tfidf_X_test = tfidf_vectorizer.transform(X_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1891a989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "#Required Library\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Instantiating SVM Classifier with Regularization Parameter, C = 1.0\n",
    "svm_classifier = SVC(C = 1.0, kernel='linear', gamma='auto')\n",
    "\n",
    "#Fitting svm_classifier to Training Data\n",
    "svm_classifier.fit(tfidf_X_train, y_train)\n",
    "\n",
    "#Saving svm_classifier for tfidf_vectorizer\n",
    "pickle.dump(svm_classifier, open(\"Downloads/svm_classifier_for_tfidf_vectorizer.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb681955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred = svm_classifier.predict(tfidf_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9e16c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine : (TF-IDF Approach) \n",
      "\n",
      "Accuracy : 98.38 %\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      "[[131   0   3   1   1]\n",
      " [  0  95   0   0   0]\n",
      " [  3   1 107   0   0]\n",
      " [  0   0   0 123   0]\n",
      " [  0   0   0   0  92]]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score & Confusion Matrix\n",
    "\n",
    "# Required Library\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Support Vector Machine : (TF-IDF Approach) \\n\")\n",
    "\n",
    "# Accuracy \n",
    "a_score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy : \" + str(\"{:.2f}\".format(a_score*100)), '%')\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "#Confusion Matrix\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, pred)\n",
    "\n",
    "print(\"Confusion Matrix : \")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a2f5de",
   "metadata": {},
   "source": [
    "# Feature Selection : Bag of Words Approah (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d235dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "# Required Library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Loading Fitted CountVectorizer\n",
    "count_vectorizer = pickle.load(open(\"Downloads/count_vectorizer.pkl\", \"rb\"))\n",
    "\n",
    "# Transformin Training Data (X_train)\n",
    "count_X_train  = count_vectorizer.transform(X_train.values)\n",
    "\n",
    "# Trainsforming Testing Data (X_test)\n",
    "count_X_test = count_vectorizer.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c9a076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector achine\n",
    "\n",
    "# Required Library\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Instantiating SVM Classifier with Regularization Parameter, C = 1.0\n",
    "svm_classifier = SVC(C=1.0, kernel='linear', gamma='auto')\n",
    "\n",
    "# Fitting svm_classifier to Training Data\n",
    "svm_classifier.fit(count_X_train, y_train)\n",
    "\n",
    "# Saving svm_classifier for count_vectorizer\n",
    "pickle.dump(svm_classifier, open(\"Downloads/svm_classifier_for_count_vectorizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d31d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred = svm_classifier.predict(count_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be56ef67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine : (BOW Approach) \n",
      "\n",
      "Accuracy : 97.49 %\n",
      "\n",
      "\n",
      "Confusion Matrix :\n",
      "[[132   0   3   0   1]\n",
      " [  2  91   0   0   2]\n",
      " [  3   0 107   1   0]\n",
      " [  0   0   1 122   0]\n",
      " [  0   1   0   0  91]]\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score & Confusion Matrix\n",
    "\n",
    "# Required Library\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Support Vector Machine : (BOW Approach) \\n\")\n",
    "\n",
    "# Accuracy\n",
    "a_score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy : \" + str(\"{:.2f}\".format(a_score*100)),'%')\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, pred)\n",
    "\n",
    "print(\"Confusion Matrix :\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b41dcb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of User News Headline\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Loading Model\n",
    "count_vectorizer = pickle.load(open(\"Downloads/tfidf_vectorizer.pkl\", \"rb\"))\n",
    "svm_classifier = pickle.load(open(\"Downloads/svm_classifier_for_tfidf_vectorizer.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7f0f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Values encoded by LabelEncoder\n",
    "encoded = {0:'Business', 1:'Entertainment', 2:'Politics', 3:'Sports', 4:'Technology'}\n",
    "\n",
    "#Input\n",
    "user_news = [input(\"Enter the news : \")]\n",
    "\n",
    "#Transformation and Prediction of user text\n",
    "news_count = count_vectorizer.transform(user_news)\n",
    "prediction = svm_classifier.predict(news_count)\n",
    "\n",
    "print(\"\\nNews Category : \", encoded[prediction[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd0a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
